{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ignore loguru logs from backend\n",
    "logger.remove()  \n",
    "logger.add(lambda msg: None, level=\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project directory to the path\n",
    "backend_path = os.path.abspath('../')\n",
    "if backend_path not in sys.path:\n",
    "    sys.path.append(backend_path)\n",
    "\n",
    "from backend.causal_models.factory import CausalLMFactory\n",
    "from backend.retriever.factory import DenseRetrieverFactory, SparseRetrieverFactory\n",
    "from backend.file_handling.extractors import PdfExtractor\n",
    "from backend.file_handling.chunker import SemanticTextChunker\n",
    "from backend.storage.chromadb import ChromaDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaDB()\n",
    "pdf_extractor = PdfExtractor()\n",
    "chunker = SemanticTextChunker()\n",
    "\n",
    "dense_retriever = DenseRetrieverFactory.get_model(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "sparse_retriever = SparseRetrieverFactory.get_model(\"bm25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from unstructured.cleaners.core import clean\n",
    "from backend.schemas import StoreEntry\n",
    "\n",
    "# List to store data for statistical analysis\n",
    "document_data = []\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = Path('./files')\n",
    "\n",
    "for pdf_file in dataset_path.glob('*.pdf'):\n",
    "    try:\n",
    "        extraced_content = pdf_extractor.extract_content(pdf_file)\n",
    "        print(f\"Extract content from {pdf_file.name}:\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while reading {pdf_file.name}: {e}.\")\n",
    "        continue\n",
    "\n",
    "    # chunk text\n",
    "    if extraced_content:\n",
    "        # Clean text\n",
    "        cleaned_text = clean(\n",
    "            extraced_content.full_text,\n",
    "            bullets=True,\n",
    "            extra_whitespace=True,\n",
    "            dashes=True,\n",
    "            trailing_punctuation=True,\n",
    "        )\n",
    "        chunked_texts = chunker.chunk_text(cleaned_text)\n",
    "\n",
    "        document_data.append({\n",
    "            'document_name': pdf_file.name,\n",
    "            #'text_length': len(cleaned_text),\n",
    "            'word_count': len(cleaned_text.split()),\n",
    "            'token_count': len(dense_retriever.tokenizer.tokenize(cleaned_text)),\n",
    "            'chunked': len(chunked_texts)\n",
    "        })\n",
    "\n",
    "        # Insert text only\n",
    "        if len(chunked_texts) > 0:\n",
    "            dense_text_vectors = dense_retriever.vectorize(chunked_texts)\n",
    "            vector_store.insert(\n",
    "                StoreEntry(\n",
    "                    type=\"text\",\n",
    "                    document_name=pdf_file.name,\n",
    "                    content=chunked_texts,\n",
    "                    vector=dense_text_vectors,\n",
    "                )\n",
    "            )\n",
    "\n",
    "         # Sparse embeddings\n",
    "        sparse_retriever.add_documents(chunked_texts)\n",
    "\n",
    "        # Insert Images with captions\n",
    "        dense_caption_vectors = dense_retriever.vectorize(\n",
    "            [img.caption for img in extraced_content.images]\n",
    "        )\n",
    "        if dense_caption_vectors is not None:\n",
    "            vector_store.insert(\n",
    "                StoreEntry(\n",
    "                    type=\"caption\",\n",
    "                    document_name=pdf_file.name,\n",
    "                    content=extraced_content.images,\n",
    "                    vector=dense_caption_vectors,\n",
    "                )\n",
    "            )\n",
    "\n",
    "df = pd.DataFrame(document_data)\n",
    "df.to_csv('document_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search and cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.schemas import SearchResult\n",
    "\n",
    "def reciprocal_rank_fusion(\n",
    "        dense_results: list[SearchResult], sparse_results: list[str], k: int = 60\n",
    "    ) -> list[SearchResult]:\n",
    "        # Store text to image mapping\n",
    "        image_mapping = dict()\n",
    "        dense_texts: list[str] = list()\n",
    "        for result in dense_results:\n",
    "            if result.image:\n",
    "                image_mapping[result.text] = result.image\n",
    "            dense_texts.append(result.text)\n",
    "\n",
    "        # Combine found docs\n",
    "        all_docs = set(sparse_results + dense_texts)\n",
    "\n",
    "        # Reciprocal Rank Fusion\n",
    "        scores = dict()\n",
    "        for doc in all_docs:\n",
    "            score = 0\n",
    "            # Calculate score contribution from sparse results\n",
    "            if doc in sparse_results:\n",
    "                rank = sparse_results.index(doc) + 1\n",
    "            else:\n",
    "                rank = len(sparse_results) + 1\n",
    "            score += 1 / (k + rank)\n",
    "\n",
    "            # Calculate score contribution from dense results\n",
    "            if doc in dense_results:\n",
    "                rank = dense_results.index(doc) + 1\n",
    "            else:\n",
    "                rank = len(dense_results) + 1\n",
    "            score += 1 / (k + rank)\n",
    "\n",
    "            scores[doc] = score\n",
    "\n",
    "        # Rank results\n",
    "        ranked_documents = sorted(scores.items(), key=lambda x: x[1], reverse=True)[\n",
    "            : 10 # Return top 10 results\n",
    "        ]\n",
    "        return [\n",
    "            SearchResult(text=doc[0], image=image_mapping.get(doc[0]))\n",
    "            for doc in ranked_documents\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Search queries\n",
    "queries = [\n",
    "    \"What time period does the sea-ice extent data cover up to?\",\n",
    "    \"Which percentage share of reneweable enery systems has europe?\",\n",
    "]\n",
    "\n",
    "# Search result cache\n",
    "search_results: list[dict[str, list[SearchResult]]] = list()\n",
    "\n",
    "# Search\n",
    "N_RESULTS = 10\n",
    "for query_index, query in enumerate(queries):\n",
    "    search_vector = dense_retriever.vectorize(query)\n",
    "    dense_results = vector_store.query(search_vector, k=N_RESULTS)\n",
    "    sparse_results = sparse_retriever.search(query, k=N_RESULTS)\n",
    "\n",
    "    results = reciprocal_rank_fusion(dense_results, sparse_results)\n",
    "\n",
    "    # Create directory for query to store images\n",
    "    query_dir = Path(f\"./images/{query_index}\")\n",
    "    query_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for image_index, result in enumerate(results):\n",
    "        if result.image:\n",
    "            # Save image\n",
    "            image_path = query_dir / f\"{image_index}.png\"\n",
    "            result.image.save(image_path)\n",
    "\n",
    "    search_results.append({\n",
    "        'query': query,\n",
    "        'results': results\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from backend.schemas import GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "            max_new_tokens=250,\n",
    "            no_repeat_ngram_size=3,\n",
    "            temperature=1.0,\n",
    "            top_k=90,\n",
    "            num_beams=3,\n",
    "            do_sample=True,\n",
    "            length_penalty=-0.7,\n",
    "        )\n",
    "\n",
    "models = [\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"HuggingFaceM4/idefics2-8b-chatty\",\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_id in models:\n",
    "    model = CausalLMFactory.get_model(model_id)\n",
    "    for search_result in search_results:\n",
    "        for res in search_result:\n",
    "            answer = model.generate(res[\"query\"], res['results'])\n",
    "            results.append({\n",
    "                \"model_id\": model_id,\n",
    "                \"query\": res[\"query\"],\n",
    "                \"results\": res['results'],\n",
    "                \"answer\": answer\n",
    "            })\n",
    "\n",
    "with open('generated_answers.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM as a judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from jinja2 import Template\n",
    "from pathlib import Path\n",
    "from backend.schemas import GenerationConfig\n",
    "from dynaconf import settings\n",
    "\n",
    "client = OpenAI(api_key=settings.OPENAI)\n",
    "\n",
    "\n",
    "judge_prompt_template = \"\"\"\n",
    "You are an impartial judge tasked with evaluating the performance of an AI assistant. Your job is to assess the assistant's response based on the given question and the provided context.\n",
    "\n",
    "Please evaluate the answer according to the following criteria:\n",
    "1. Did the assistant correctly answer the question based on the information in the context?\n",
    "2. If the context did not contain sufficient information, did the assistant clearly communicate this?\n",
    "\n",
    "Scoring guidelines:\n",
    "- Award 1 point if the answer is correct or if the assistant appropriately communicated that the context was insufficient.\n",
    "- Award 0 points if the answer is incorrect or if the assistant erroneously attempted to answer the question despite insufficient context.\n",
    "\n",
    "Question: {{ query }}\n",
    "Context: {{ context }}\n",
    "Assistant's Answer: {{ answer }}\n",
    "\n",
    "Please provide your evaluation in the following JSON format. Ensure that the output is valid JSON:\n",
    "\n",
    "{\n",
    "  \"score\": [0 or 1],\n",
    "  \"reasoning\": \"Your detailed justification for the score\",\n",
    "  \"feedback\": \"Constructive feedback or suggestions for improvement for the assistant\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_results(results):\n",
    "    evaluations = []\n",
    "    template = Template(judge_prompt_template)\n",
    "    \n",
    "    for result in results:\n",
    "        prompt = template.render(\n",
    "            query=result[\"query\"],\n",
    "            context=result[\"context\"],\n",
    "            answer=result[\"answer\"]\n",
    "        )\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "            ],\n",
    "            max_tokens=150,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        evaluation = json.loads(response.choices[0].message['content'].strip()) # TODO\n",
    "        evaluations.append(evaluation)\n",
    "    \n",
    "    return evaluations\n",
    "\n",
    "evaluations = evaluate_results(saved_results)\n",
    "\n",
    "with open(\"evaluations.json\", \"w\") as f:\n",
    "    json.dump(evaluations, f, indent=4)\n",
    "\n",
    "print(\"Bewertungen gespeichert.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
